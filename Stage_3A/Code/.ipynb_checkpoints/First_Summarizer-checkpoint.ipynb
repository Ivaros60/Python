{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lien vers le site\n",
    "\n",
    "https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1\n",
    "\n",
    "# Premier summarizer\n",
    "\n",
    "Je vais commencer par faire un premier type de summarizer, simple, basique, peu efficace, dans le but d'avoir une première base de travail. \n",
    "\n",
    "Ce dernier se décomposera en plusieurs étapes (7 précisément) et chacune des étapes fera office de partie au sein de ce notebook.\n",
    "\n",
    "## Importation du'un modèle pour la langue francaise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "path = \"C:\\\\Users\\\\ivaro\\\\OneDrive\\\\Bureau\\\\NLP\\\\\" #A modifier/automatiser\n",
    "nlp = spacy.load(path + \"fr_core_news_sm-2.2.5\") #Charge le modele fr deja installe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\ivaro\\\\OneDrive\\\\Bureau\\\\Stage_3A\\\\Debat\\\\debat_2007.txt', 'r', encoding = \"utf-8\") as file:\n",
    "     texte = file.read()\n",
    "        \n",
    "with open('C:\\\\Users\\\\ivaro\\\\OneDrive\\\\Bureau\\\\Stage_3A\\\\Debat\\\\Interview_Prelats.txt', 'r', encoding = \"utf-8\") as file:\n",
    "     prelat = file.read()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "\n",
    "Maintenant que j'ai récupéré le texte à résumer, je vais chercher à déterminer l'importance d'une phrase pour cela je commence par tokeniser mon texte phrase par phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_token_sent(texte):\n",
    "    \"\"\"Tokenisation du texte par phrase\"\"\"\n",
    "    doc = nlp(texte)\n",
    "    # Retourner le texte de chaque phrase\n",
    "    return [X.text for X in doc.sents]\n",
    "\n",
    "Tokenize = return_token_sent(texte)\n",
    "len(Tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien une séparation par phrase excepté au début, avec le Bonsoir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation \n",
    "\n",
    "### Vectorisation mot par mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def return_word_embedding(sentence):\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le vecteur lié à chaque token\n",
    "    return [(X.vector) for X in doc]\n",
    "\n",
    "Sentence_emb0 = return_word_embedding(Tokenize[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la première phrase on constate 7 mots dont le point et les deux points. A supprimer ainsi que les stopwords par la suite.\n",
    "\n",
    "### Vectorisation par phrase\n",
    "\n",
    "On va \"moyenner\" les vecteurs d'une meme phrase pour pouvoir avoir une représentation vectorielle d'une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sentence_emb(sentence):\n",
    "    \"\"\"Prend en entree une phrase vectorise mot a mot\"\"\"\n",
    "    somme = sentence[0]\n",
    "    for i in range(len(sentence)) :\n",
    "        if i != 0 :\n",
    "            somme += sentence[i]\n",
    "    return somme/len(sentence)\n",
    "\n",
    "Vect = return_sentence_emb(Sentence_emb0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_text_emb(texte) : \n",
    "    \"\"\"Vectorisation du texte\"\"\"\n",
    "    text_vect = []\n",
    "    Tokenize = return_token_sent(texte) #Tokenisation par phrase    \n",
    "    for i in range(len(Tokenize)) :\n",
    "        Sentence_emb = return_word_embedding(Tokenize[i]) #Vectorisation des phrases\n",
    "        vect = return_sentence_emb(Sentence_emb)\n",
    "        text_vect += [vect]\n",
    "    return text_vect\n",
    "\n",
    "#Test = return_text_emb(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterisation\n",
    "\n",
    "Maintenant on va effectuer le Kmeans. Tout d'abord on va choisir le nombre de centre. Dans notre configuration on va avoir, un centre = une phrase. Comme on souhaite faire un résumé, on peut choisir de conserver X% du texte initial. Par exemple 10%. C'est ce nombre que l'on va conserver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-5623dc14377e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnb_centre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.10\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_token_sent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#partie entiere avec np.ceil qui renvois un flottant -> int pour le kmeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_centre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-6ebb569cf804>\u001b[0m in \u001b[0;36mreturn_token_sent\u001b[1;34m(texte)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreturn_token_sent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Tokenisation du texte par phrase\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Retourner le texte de chaque phrase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ivaro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "nb_phrase = len(Tokenize)\n",
    "\n",
    "nb_centre = int(np.ceil(0.10*nb_phrase)) \n",
    "#partie entiere avec np.ceil qui renvois un flottant -> int pour le kmeans\n",
    "kmeans = KMeans(n_clusters=nb_centre)\n",
    "kmeans = kmeans.fit(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(kmeans.labels_) #Pour avoir le numero du cluster attribue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chacun l'a observé. 25 ans. 45 %! Bonsoir Ségolène Royal, bonsoir, Nicolas Sarkozy. Ségolène Royal: Sur l'hôpital public, qui est une question cruciale, essentielle, puisque c'est aussi sur le maintien des petits hôpitaux, la présence de santé sur l'ensemble du territoire national. Si vous augmentez les dépenses, vous augmentez les impôts. Je propose une chose: 45 % du budget de la France, c'est le salaire de la fonction publique et les pensions de retraite. Voulez-vous aussi dire que vous allez supprimer des poste de magistrats alors qu'il n'y a aucune réponse pour les jeunes délinquants...\\n\\nNicolas Sarkozy: Mme Royal dit qu'elle va transférer aux régions, ce qui leur permettra de faire une autre augmentation après les autres augmentations faramineuses que vous avez faites les années passées. Il y aura donc, si je suis président de la République, je veux proposer aux fonctionnaires un pacte de progrès. Si la croissance est supérieure à 2,5%, à la fois, je finance mon pacte présidentiel, j'ai donc relancé la croissance en mettant l'accent sur les PME. Le commissariat de Clichy que vous avez promis, il n'est pas ouvert. On remplacera un départ sur deux à la retraite, la moitié des gains de productivité permettant d'augmenter les salaires des fonctionnaires, parce que les salaires de la fonction publique sont très bas. Arlette Chabot: Sur la croissance, si vous voulez répondre...\\n\\nNicolas Sarkozy: Mme Royal ne m'en voudra pas, mais a évoqué tous les sujets en même temps, elle risque de les survoler et de ne pas être assez précise. Tenez-vous bien, entre temps, les effectifs de l'Etat sur les compétences sociales ont été multipliés par quatre. Vous avez dit : où trouve-t-on les économies de fonctionnaires ? S'il y avait davantage de policiers, peut-être que cette femme n'aurait pas été violée, car elle n'aurait pas été seule, elle serait rentrée chez elle avec un collègue. C'est sympathique. Je demande que les pédophiles, en particulier, ne soient pas relâchés tant que la commission spéciale qui aurait dû être mise en place dans les prisons n'a pas formellement dit par expertise qu'ils sont désormais non nocifs. Au moins sur ce choix-là, nous ne pourrions pas, gauche et droite, opposition et majorité, selon le  choix des Français, nous trouver d'accord pour dire, bien sûr, si on veut soulager la dette des Français qui est injuste pour les générations qui viennent, il faudra faire des économies et, les grosses économies, on les fait sur les gros postes de dépense.Ségolène Royal: Je voudrais revenir sur la conception du pouvoir, car c'est important sur cet engagement qui consiste à tout chef d'Etat demain de rendre des comptes sur son pouvoir passé. Je pense que la France a la capacité de réaliser un taux de croissance de 2,5%. Il faut aller plus loin. Ségolène Royal, comment vous sentez-vous? Ségolène Royal: Si vous ne pouvez pas faire, pourquoi voulez-vous accéder aux responsabilités? Or, aujourd'hui, il n'y a pas de réponse, vous le savez, au premier acte de délinquance. Il y a un deuxième point où l’on est en accord, il faut résoudre le problème de la dette. Dites-moi, si vous devez faire raccompagner toutes les femmes fonctionnaires chez elles la nuit…\\n\\nSégolène Royal: Parfaitement! Pourquoi? J'ai dit que je maintenais leur nombre, mais que je les redéploierai en les retirant là où ils ne sont plus nécessaires…\\n\\nNicolas Sarkozy: Vous ne pouvez pas passer d'une fonction publique à une autre. Les centres éducatifs renforcés seront effectivement créés. Permettez que je vous pose la question. Par quel moyen? Comment? D'accord, pourquoi pas. Ségolène Royal: Pourquoi pas ? C'est emploi tremplin.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "avg = []\n",
    "for j in range(nb_centre):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    avg.append(np.mean(idx))\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, Test)\n",
    "ordering = sorted(range(nb_centre), key=lambda k: avg[k])\n",
    "summary = ' '.join([Tokenize[closest[idx]] for idx in ordering])\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clairement à améliorer.\n",
    "\n",
    "- Le decoupage/tokenisation n'est pas parfait et loin de la. \n",
    "- On peut supprimer la ponctuation au sein d'une phrase car elle n'apporte rien et rentre en compte par la suite dans l'embedding ? Mauvaise idée car la ponctuation peut finalement etre utile.\n",
    "- On peut supprimer les stopwords qui n'apportent pas d'information ou peu.\n",
    "- Ici on a affaire à une interview/debat. On peut donc localiser les entités nommées. (Utile pour le format réunion). \n",
    "- Localisation des différents sujets pour résumer par sujet. \n",
    "- La méthode d'embedding des phrases est brute et surement pas adaptée. Pensez à pondérer selon l'importance du mot ?\n",
    "- Summarizer ? pas ouf....\n",
    "- pk une dimension 32 dans l'embeding des mots\n",
    "- ajouter les formules de politesse aux stopwords / les retirer dans tous les cas car n'apportent pas de sens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Améliorations\n",
    "\n",
    "### Sur la Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pour ce nouveau PotterAfter, nous vous proposons de découvrir une saga littéraire entièrement financée via un site de financement participatif.',\n",
       " 'Aujourd’hui, je viens vous parler d’une saga d’heroic fantasy découverte lors de l’édition 2019 du Salon du Livre de Paris, Les Prélats de Faneas de Charlotte Abécassis Weigel.',\n",
       " 'Elle se compose pour l’instant de trois romans, Les terres d’exil, Le soulèvement des fiefs et L’alliance d’Amiran, tous publiés grâce à un site de financement participatif.',\n",
       " 'J’ai d’abord été attirée par la couverture très sobre du premier tome, un petit renard stylisé sur fond bleu, puis une fois le livre en main, j’ai tout de suite été captivée par les trois premières phrases :\\n\\n«Vous qui n’avez ni dieu, ni pouvoir.',\n",
       " 'Vous qui avez abandonné vos rêves, les prenant pour des chimères.',\n",
       " 'Ne vous êtes-vous jamais demandé où naissent les récits de vos anciennes légendes?»\\n\\nL’HISTOIRE\\nNous suivons les aventures d’Astéria et de sa cousine Cléora, envoyées sur Terre, par leur roi, après que leur royaume a été envahi par le royaume rival d’Endor.',\n",
       " 'Elles n’ont pas été envoyées dans notre monde par hasard, elles ont une mission : trouver les deux Prélats disparus, ceux de la terre et du feu.',\n",
       " 'Car à Faneas, pour chaque roi amené à gouverner, il existe quatre Prélats, des êtres dotés de pouvoirs liés aux quatre éléments qui, ensemble, auraient le pouvoir de vaincre même les dieux.',\n",
       " 'La quête des deux jeunes femmes est donc primordiale quant à la survie de leur royaume.',\n",
       " 'Après des mois de recherche sans succès, elles finissent par attirer l’attention de deux jeunes hommes, Noctis et Klay.',\n",
       " 'Potentiels alliés ou ennemis, les deux cousines devront s’armer de courage pour découvrir tous les secrets qu’ils cachent et ainsi espérer sauver leur terre natale.',\n",
       " 'POURQUOI VOUS PROPOSER CES ROMANS ?',\n",
       " 'Tout d’abord pour l’univers très riche, fortement inspiré de la mythologie, où il est facile de se perdre, ainsi que le style d’écriture fluide et agréable à lire.',\n",
       " 'Les descriptions sont claires et précises, il est donc aisé d’imaginer les différentes provinces du royaume des héroïnes.',\n",
       " 'Dès la toute première page, nous sommes plongés dans les récits ancestraux qui relatent la création de Faneas et l’apparition des Prélats.',\n",
       " 'Au fil de la lecture, on découvre un univers complexe, divisé en quatre contrées, basé sur l’équilibre des quatre éléments, la terre, l’eau, l’air et le feu.',\n",
       " 'On s’attache très vite aux personnages, malgré le tempérament de feu d’Astéria, qui a tendance à foncer tête baissée, sans se soucier des conséquences (dans le premier tome, du moins).',\n",
       " 'On aura tendance à penser qu’il est difficile de s’attacher à un personnage avec tel caractère et pourtant, elle en devient admirable pour le dévouement qu’elle montre envers son pays.',\n",
       " 'Décrite comme une guerrière hors pair, elle reste avant tout une jeune femme un peu naïve, très à l’aise lorsqu’il s’agit de combat, mais beaucoup moins en présence de représentants du sexe opposé.',\n",
       " 'Cléora, quant à elle, a un tempérament à l’opposé de celui de sa cousine.',\n",
       " 'Elle est décrite comme beaucoup plus mesurée et déterminée, mais elle n’en reste pas moins une redoutable guerrière.',\n",
       " 'Bien qu’ayant des caractères contradictoires, les deux femmes se complètent et parviennent à surmonter les obstacles qui pourraient se trouver en travers de leur chemin.',\n",
       " 'J’ai beaucoup apprécié le fait qu’il n’y ait pas qu’un seul point de vue ; nous avons tour à tour accès aux pensées de chaque personnage, ce qui nous permet de connaître leurs intentions… ou presque.',\n",
       " 'En effet, l’autrice prend un malin plaisir à mener ses personnages, ainsi que ses lecteurs en bateau.',\n",
       " 'On comprend rapidement que chaque petit détail a son importance et le roman se transforme ainsi en une véritable chasse aux énigmes.',\n",
       " 'Le premier tome de la saga (qui doit en compter quatre), pose les bases de cet univers très semblable et en même temps très éloigné du nôtre, pour ensuite nous faire basculer au cœur de l’intrigue dans les tomes suivants.',\n",
       " 'Les véritables natures de chacun se précisent et les enjeux se font plus importants, ce qui nous plonge dans une véritable course contre la montre.',\n",
       " 'L’INFLUENCE DE L’OEUVRE DE J. K. ROWLING\\nL’autrice le dit elle-même sur son site, «par certains aspects, cet univers pourrait plaire à ceux qui auraient aimé Harry Potter, Le Seigneur des anneaux, Twilight, Eragon… et beaucoup d’autres.» Car en plus de nous plonger dans un univers fantastique qui cache de multiples secrets, les livres de Charlotte Abécassis Weigel sont empreints de magie, de suspens, de trahisons… Et quoi de mieux pour avoir des informations sur un roman que d’avoir les explications de son autrice ?',\n",
       " 'Comme nous ne faisons jamais les choses à moitié à la Gazette, nous avons contacté Charlotte Abécassis Weigel, qui a eu la gentillesse de répondre à quelques questions.',\n",
       " 'ENTRETIEN\\nVous dites que c’est la saga de J.K.',\n",
       " 'Rowling qui vous a donné envie d’écrire, quelle place tient-elle dans votre vie, aujourd’hui ?',\n",
       " 'Je ne lisais pas beaucoup avant Harry Potter.',\n",
       " 'J’étais d’ailleurs persuadée de ne pas aimer ça, mais en fait, je n’avais pas encore trouvé le style qui m’intéressait, à savoir la fantasy.',\n",
       " 'Mais même après avoir lu d’autres livres fantastiques, Harry Potter restait mon préféré, notamment parce que l’écriture de J.K.',\n",
       " 'Rowling est particulièrement fluide et émotive.',\n",
       " 'Je pouvais rire en lisant les descriptions du cou de Pétunia ou les maladresses d’Hagrid, et je pouvais frissonner en découvrant les détraqueurs, l’histoire mélancolique des parents de Harry ou encore le sort réservé à Buck.',\n",
       " 'J’ai tellement aimé qu’à la fin du tome 5, lorsqu’il a fallu attendre un an avant la prochaine sortie, je n’ai rien trouvé à me mettre sous la dent.',\n",
       " 'J’ai alors découvert les fan-fictions, des histoires écrites par des fans que l’on peut trouver sur Internet.',\n",
       " 'Il y en a beaucoup sur Harry Potter et j’ai tout de suite été séduite par l’accueil qu’elles recevaient.',\n",
       " 'Car même s’il s’agit d’un travail d’amateur et qu’elles ne sont souvent pas à la hauteur d’un roman, elles sont toujours les bienvenues.',\n",
       " 'Les lecteurs en discutent avec bienveillance, donnant des conseils et des critiques constructives.',\n",
       " 'J’ai été séduite par cette façon d’écrire et je me suis lancée.',\n",
       " 'J’ai commencé en écrivant sur Harry Potter, puis, sans même m’en rendre compte, j’ai commencé à inventer mon univers.',\n",
       " 'La mythologie grecque est omniprésente dans vos livres, (noms des personnages, prophéties…), pourquoi ?',\n",
       " 'La mythologie de façon générale est très présente dans mes livres, notamment parce que je trouve les légendes qui entourent ces thèmes, passionnantes.',\n",
       " 'J’aime faire des parallèles entre les dieux et surtout, j’aime le côté retord des prophéties contre lesquelles, nous pauvres mortels, ne pouvons pas grand-chose.',\n",
       " 'J’aime beaucoup l’influence des dieux sur les hommes et je trouve que les mythologies sont une source inépuisable de leçons et de morale : une trame de fond intéressante pour un livre !',\n",
       " 'Pourquoi une plateforme de financement participatif plutôt qu’une maison d’édition traditionnelle ?',\n",
       " 'Mes livres sortent effectivement via des financements participatifs sur la plateforme Ulule et non grâce à une maison d’édition.',\n",
       " 'Les raisons sont diverses.',\n",
       " 'Premièrement, je n’avais pas très envie de passer par une maison d’édition.',\n",
       " 'Je ne savais pas très bien comment cela fonctionnait et j’avais entendu que c’était un processus très long.',\n",
       " 'D’autre part, du fait de mon métier de graphiste, je voulais m’occuper moi-même de la partie communication.',\n",
       " 'En plus, mon mari et moi avions décidé de mener ce projet de publication ensemble.',\n",
       " 'Mais sans ressource et sans savoir si Les Prélats de Faneas susciterait de l’intérêt auprès des lecteurs, nous ne savions pas par où commencer.',\n",
       " 'Nous avons alors décidé de lancer notre premier financement participatif.',\n",
       " 'Le refaire les années suivantes nous a semblé une évidence.',\n",
       " 'C’est du gagnant-gagnant puisque l’auteur génère les fonds nécessaires pour donner vie au projet et les participants reçoivent les produits en avant-première et bénéficient souvent d’une promotion.',\n",
       " 'De plus, c’est un temps fort dans la vie du livre, un moment d’échange et de partage, entre autre grâce aux cadeaux bonus débloqués à chaque fois que l’on dépasse un nouveau palier.',\n",
       " 'C’est ainsi, par exemple, que les participants aux dernier Ulule ont débloqué un chapitre bonus, dévoilé à Noël.',\n",
       " 'Qu’éprouvez-vous avant la sortie d’un roman ?',\n",
       " 'Crainte, réjouissance ?',\n",
       " 'Et après ?',\n",
       " 'Un peu de tout j’imagine.',\n",
       " 'J’ai un grand manque de confiance en moi, ce qui me pousse avoir toujours le pire.',\n",
       " 'Souvent, on me dit que les couvertures de mes livres sont très belles, ce qui me fait plaisir car en plus d’écrire, je les ai dessinées moi-même et j’en suis fière.',\n",
       " 'Mais au lieu de laisser cela s’exprimer, ma première réaction est de m’inquiéter du fait qu’un lecteur, s’il achète le livre parce que la couverture lui plait, puisse être déçu par l’histoire.',\n",
       " 'Heureusement, avec le temps, j’ai appris que ce n’était pas le cas et que de façon générale, mon livre plaisait vraiment à son lectorat.',\n",
       " 'Je peux donc dire à présent que je suis fière et heureuse de présenter mes livres, car je sais qu’ils rendent les lecteurs heureux.',\n",
       " 'C’est une sensation vraiment magique de savoir que quelque chose que vous avez imaginé donne du plaisir aux gens, qu’ils ont ri ou pleuré en découvrant la suite.',\n",
       " 'Je ne m’étais jamais sentie aussi utile, avant, et je ne remercierai jamais assez mes lecteurs pour ça !']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Seconde methode pour Tokeniser. \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download(\"punkt\")\n",
    "Tokenize = sent_tokenize(prelat, language = \"french\")\n",
    "Tokenize\n",
    "#len(Tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenisation semble de meilleurs qualités !\n",
    "\n",
    "## Suppression des StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour supprimer les stopWords \n",
    "stopWords = set(stopwords.words('french'))\n",
    "\n",
    "def delete_SW(test,stopWords) :\n",
    "    clean_words = []\n",
    "    for token in return_token(test):\n",
    "        if token not in stopWords:\n",
    "            clean_words.append(token)\n",
    "    return clean_words\n",
    "\n",
    "clean_words = delete_SW(test)\n",
    "clean_words\n",
    "\n",
    "#Ajout d'elements a SW\n",
    "\n",
    "def add_SW(L,SW) :\n",
    "    \"\"\"Ajoute une liste d element\"\"\"\n",
    "    for j in L:\n",
    "       SW.add(j) \n",
    "    return SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relancer la fonction de resume avec les cgts\n",
    "\n",
    "def return_text_emb(texte) : \n",
    "    \"\"\"Vectorisation du texte\"\"\"\n",
    "    text_vect = []\n",
    "    Tokenize = sent_tokenize(texte, language = \"french\") #Tokenisation par phrase    \n",
    "    for i in range(len(Tokenize)) :\n",
    "        Sentence_emb = return_word_embedding(Tokenize[i]) #Vectorisation des phrases\n",
    "        vect = return_sentence_emb(Sentence_emb)\n",
    "        text_vect += [vect]\n",
    "    return text_vect\n",
    "\n",
    "Test = return_text_emb(prelat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POURQUOI VOUS PROPOSER CES ROMANS ? L’INFLUENCE DE L’OEUVRE DE J. K. ROWLING\\nL’autrice le dit elle-même sur son site, «par certains aspects, cet univers pourrait plaire à ceux qui auraient aimé Harry Potter, Le Seigneur des anneaux, Twilight, Eragon… et beaucoup d’autres.» Car en plus de nous plonger dans un univers fantastique qui cache de multiples secrets, les livres de Charlotte Abécassis Weigel sont empreints de magie, de suspens, de trahisons… Et quoi de mieux pour avoir des informations sur un roman que d’avoir les explications de son autrice ? Elles n’ont pas été envoyées dans notre monde par hasard, elles ont une mission : trouver les deux Prélats disparus, ceux de la terre et du feu. J’ai d’abord été attirée par la couverture très sobre du premier tome, un petit renard stylisé sur fond bleu, puis une fois le livre en main, j’ai tout de suite été captivée par les trois premières phrases :\\n\\n«Vous qui n’avez ni dieu, ni pouvoir. Premièrement, je n’avais pas très envie de passer par une maison d’édition. J’ai tellement aimé qu’à la fin du tome 5, lorsqu’il a fallu attendre un an avant la prochaine sortie, je n’ai rien trouvé à me mettre sous la dent. Un peu de tout j’imagine. Je ne m’étais jamais sentie aussi utile, avant, et je ne remercierai jamais assez mes lecteurs pour ça ! Les raisons sont diverses. Crainte, réjouissance ? Et après ?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "nb_phrase = len(Tokenize)\n",
    "\n",
    "nb_centre = int(np.ceil(0.15*nb_phrase)) \n",
    "#partie entiere avec np.ceil qui renvois un flottant -> int pour le kmeans\n",
    "kmeans = KMeans(n_clusters=nb_centre)\n",
    "kmeans = kmeans.fit(Test)\n",
    "\n",
    "avg = []\n",
    "for j in range(nb_centre):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    avg.append(np.mean(idx))\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, Test)\n",
    "ordering = sorted(range(nb_centre), key=lambda k: avg[k])\n",
    "summary = ' '.join([Tokenize[closest[idx]] for idx in ordering])\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pondération de la phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher a calculer la frequence d'apparition de chaque mot dans notre texte pour donner de l importance aux mots \"rares\"\n",
    "\n",
    "### Obtention de la fréquence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['retent',\n",
       " '.',\n",
       " 'encor',\n",
       " '.',\n",
       " 'et',\n",
       " 'encor',\n",
       " '.',\n",
       " 'voir',\n",
       " 'mem',\n",
       " 'encor',\n",
       " 'une',\n",
       " 'fois',\n",
       " '.',\n",
       " 'et',\n",
       " 'une',\n",
       " 'dernier',\n",
       " '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Récupération des racines de mots\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='french')\n",
    "\n",
    "def return_stem(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [stemmer.stem(X.text) for X in doc] #X.text correspond au mot\n",
    "\n",
    "return_stem(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obt_occ(dictionnaire,stopwords,mot) :\n",
    "    \"\"\"Renvoie le nombre d'occurence du mot \"\"\"\n",
    "    if mot in stopwords :\n",
    "        return 0\n",
    "    else : \n",
    "        if (mot in dictionnaire) :\n",
    "            return 1/dictionnaire[mot]\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pour': 12,\n",
       " 'ce': 9,\n",
       " 'nouveau': 2,\n",
       " 'potteraft': 1,\n",
       " ',': 114,\n",
       " 'nous': 14,\n",
       " 'vous': 11,\n",
       " 'proposon': 1,\n",
       " 'de': 100,\n",
       " 'découvr': 5,\n",
       " 'une': 18,\n",
       " 'sag': 4,\n",
       " 'littérair': 1,\n",
       " 'entier': 1,\n",
       " 'financ': 6,\n",
       " 'vi': 5,\n",
       " 'un': 24,\n",
       " 'sit': 3,\n",
       " 'particip': 7,\n",
       " '.': 60,\n",
       " '\\n\\n\\n': 1,\n",
       " 'aujourd’hui': 2,\n",
       " 'je': 18,\n",
       " 'vien': 1,\n",
       " 'parl': 1,\n",
       " 'd’': 29,\n",
       " 'heroic': 1,\n",
       " 'fantasy': 2,\n",
       " 'découvert': 2,\n",
       " 'lor': 1,\n",
       " 'l’': 26,\n",
       " 'édit': 4,\n",
       " '2019': 1,\n",
       " 'du': 15,\n",
       " 'salon': 1,\n",
       " 'livr': 13,\n",
       " 'paris': 1,\n",
       " 'le': 64,\n",
       " 'prélat': 5,\n",
       " 'fan': 6,\n",
       " 'charlott': 3,\n",
       " 'abécass': 3,\n",
       " 'weigel': 3,\n",
       " '\\n': 20,\n",
       " 'elle': 15,\n",
       " 'se': 10,\n",
       " 'compos': 1,\n",
       " 'instant': 1,\n",
       " 'trois': 2,\n",
       " 'roman': 6,\n",
       " 'terr': 5,\n",
       " 'exil': 1,\n",
       " 'soulev': 1,\n",
       " 'fief': 1,\n",
       " 'et': 38,\n",
       " 'allianc': 1,\n",
       " 'amiran': 1,\n",
       " 'tous': 2,\n",
       " 'publi': 1,\n",
       " 'grâc': 3,\n",
       " 'à': 29,\n",
       " 'j’': 18,\n",
       " 'ai': 13,\n",
       " 'abord': 2,\n",
       " 'été': 6,\n",
       " 'attir': 2,\n",
       " 'par': 14,\n",
       " 'la': 27,\n",
       " 'couvertur': 3,\n",
       " 'tres': 11,\n",
       " 'sobr': 1,\n",
       " 'premi': 8,\n",
       " 'tom': 5,\n",
       " 'pet': 2,\n",
       " 'renard': 1,\n",
       " 'stylis': 1,\n",
       " 'sur': 10,\n",
       " 'fond': 3,\n",
       " 'bleu': 1,\n",
       " 'puis': 2,\n",
       " 'fois': 2,\n",
       " 'en': 25,\n",
       " 'main': 1,\n",
       " 'tout': 6,\n",
       " 'suit': 3,\n",
       " 'captiv': 1,\n",
       " 'phras': 1,\n",
       " ':': 3,\n",
       " '\\n\\n': 12,\n",
       " '«': 2,\n",
       " 'qui': 17,\n",
       " 'n’': 8,\n",
       " 'avez': 3,\n",
       " 'ni': 2,\n",
       " 'dieu': 1,\n",
       " 'pouvoir': 3,\n",
       " 'abandon': 1,\n",
       " 'vos': 3,\n",
       " 'rêv': 1,\n",
       " 'pren': 1,\n",
       " 'chimer': 1,\n",
       " 'ne': 10,\n",
       " 'ête': 1,\n",
       " '-': 8,\n",
       " 'jam': 4,\n",
       " 'demand': 1,\n",
       " 'où': 3,\n",
       " 'naissent': 1,\n",
       " 'récit': 2,\n",
       " 'ancien': 1,\n",
       " 'légend': 2,\n",
       " '?': 9,\n",
       " '»': 2,\n",
       " 'histoir': 4,\n",
       " 'suivon': 1,\n",
       " 'aventur': 1,\n",
       " 'astéri': 2,\n",
       " 'sa': 2,\n",
       " 'cousin': 3,\n",
       " 'cléor': 2,\n",
       " 'envoi': 2,\n",
       " 'leur': 6,\n",
       " 'roi': 2,\n",
       " 'apres': 4,\n",
       " 'que': 20,\n",
       " 'royaum': 4,\n",
       " 'a': 9,\n",
       " 'envah': 1,\n",
       " 'rival': 1,\n",
       " 'endor': 1,\n",
       " 'ont': 4,\n",
       " 'pas': 12,\n",
       " 'dan': 10,\n",
       " 'notr': 2,\n",
       " 'mond': 1,\n",
       " 'hasard': 1,\n",
       " 'mission': 1,\n",
       " 'trouv': 7,\n",
       " 'deux': 5,\n",
       " 'disparus': 1,\n",
       " 'ceux': 2,\n",
       " 'feu': 3,\n",
       " 'car': 5,\n",
       " 'chaqu': 4,\n",
       " 'amen': 1,\n",
       " 'gouvern': 1,\n",
       " 'il': 13,\n",
       " 'exist': 1,\n",
       " 'quatr': 5,\n",
       " 'être': 2,\n",
       " 'dot': 1,\n",
       " 'li': 1,\n",
       " 'aux': 7,\n",
       " 'élément': 2,\n",
       " 'ensembl': 2,\n",
       " 'aur': 3,\n",
       " 'vaincr': 1,\n",
       " 'mêm': 8,\n",
       " 'dieux': 3,\n",
       " 'quêt': 1,\n",
       " 'jeun': 3,\n",
       " 'femm': 3,\n",
       " 'est': 14,\n",
       " 'donc': 3,\n",
       " 'primordial': 1,\n",
       " 'quant': 2,\n",
       " 'surv': 1,\n",
       " 'mois': 1,\n",
       " 'recherch': 1,\n",
       " 'san': 5,\n",
       " 'succes': 1,\n",
       " 'fin': 2,\n",
       " 'attent': 1,\n",
       " 'homm': 2,\n",
       " 'noct': 1,\n",
       " 'klay': 1,\n",
       " 'potentiel': 1,\n",
       " 'alli': 1,\n",
       " 'ou': 5,\n",
       " 'ennem': 1,\n",
       " 'devront': 1,\n",
       " 's’': 8,\n",
       " 'armer': 1,\n",
       " 'courag': 1,\n",
       " 'secret': 2,\n",
       " 'qu’': 14,\n",
       " 'cachent': 1,\n",
       " 'ains': 5,\n",
       " 'esper': 1,\n",
       " 'sauv': 1,\n",
       " 'natal': 1,\n",
       " 'pourquoi': 3,\n",
       " 'propos': 1,\n",
       " 'univer': 6,\n",
       " 'rich': 1,\n",
       " 'fort': 2,\n",
       " 'inspir': 1,\n",
       " 'mytholog': 4,\n",
       " 'facil': 1,\n",
       " 'perdr': 1,\n",
       " 'styl': 2,\n",
       " 'écritur': 2,\n",
       " 'fluid': 2,\n",
       " 'agréabl': 1,\n",
       " 'lir': 1,\n",
       " 'descript': 2,\n",
       " 'sont': 7,\n",
       " 'clair': 1,\n",
       " 'précis': 1,\n",
       " 'ais': 2,\n",
       " 'imagin': 3,\n",
       " 'différent': 1,\n",
       " 'provinc': 1,\n",
       " 'héroïn': 1,\n",
       " 'des': 1,\n",
       " 'pag': 1,\n",
       " 'somm': 1,\n",
       " 'plong': 3,\n",
       " 'ancestral': 1,\n",
       " 'relatent': 1,\n",
       " 'création': 1,\n",
       " 'apparit': 1,\n",
       " 'au': 4,\n",
       " 'fil': 1,\n",
       " 'lectur': 1,\n",
       " 'on': 7,\n",
       " 'complex': 1,\n",
       " 'divis': 1,\n",
       " 'contr': 3,\n",
       " 'bas': 2,\n",
       " 'équilibr': 1,\n",
       " 'eau': 1,\n",
       " 'air': 1,\n",
       " 'attach': 2,\n",
       " 'vit': 1,\n",
       " 'personnag': 5,\n",
       " 'malgr': 1,\n",
       " 'temper': 2,\n",
       " 'tendanc': 2,\n",
       " 'fonc': 1,\n",
       " 'têt': 1,\n",
       " 'baiss': 1,\n",
       " 'souci': 1,\n",
       " 'conséquent': 1,\n",
       " '(': 3,\n",
       " 'moin': 3,\n",
       " ')': 3,\n",
       " 'pens': 2,\n",
       " 'difficil': 1,\n",
       " 'avec': 3,\n",
       " 'tel': 2,\n",
       " 'caracter': 2,\n",
       " 'pourt': 1,\n",
       " 'devient': 1,\n",
       " 'admir': 1,\n",
       " 'dévou': 1,\n",
       " 'montr': 2,\n",
       " 'enver': 1,\n",
       " 'son': 5,\n",
       " 'pay': 1,\n",
       " 'décrit': 2,\n",
       " 'comm': 3,\n",
       " 'guerri': 2,\n",
       " 'hor': 1,\n",
       " 'pair': 1,\n",
       " 'rest': 3,\n",
       " 'avant': 5,\n",
       " 'peu': 2,\n",
       " 'naïv': 1,\n",
       " 'lorsqu’': 2,\n",
       " 'agit': 2,\n",
       " 'combat': 1,\n",
       " 'mais': 6,\n",
       " 'beaucoup': 7,\n",
       " 'présenc': 1,\n",
       " 'représent': 1,\n",
       " 'sex': 1,\n",
       " 'oppos': 2,\n",
       " 'celui': 1,\n",
       " 'plus': 6,\n",
       " 'mesur': 1,\n",
       " 'détermin': 1,\n",
       " 'redout': 1,\n",
       " 'bien': 2,\n",
       " 'ayant': 1,\n",
       " 'contradictoir': 1,\n",
       " 'complètent': 1,\n",
       " 'parviennent': 1,\n",
       " 'surmont': 1,\n",
       " 'obstacl': 1,\n",
       " 'pourr': 2,\n",
       " 'traver': 1,\n",
       " 'chemin': 1,\n",
       " 'appréci': 1,\n",
       " 'fait': 5,\n",
       " 'y': 2,\n",
       " 'ait': 1,\n",
       " 'seul': 1,\n",
       " 'point': 1,\n",
       " 'vu': 1,\n",
       " ';': 1,\n",
       " 'avon': 3,\n",
       " 'tour': 2,\n",
       " 'acces': 1,\n",
       " 'permet': 1,\n",
       " 'connaîtr': 1,\n",
       " 'intent': 1,\n",
       " '…': 4,\n",
       " 'presqu': 1,\n",
       " 'effet': 1,\n",
       " 'autric': 3,\n",
       " 'prend': 1,\n",
       " 'malin': 1,\n",
       " 'plais': 4,\n",
       " 'men': 2,\n",
       " 'lecteur': 6,\n",
       " 'bateau': 1,\n",
       " 'comprend': 1,\n",
       " 'rapid': 1,\n",
       " 'détail': 1,\n",
       " 'import': 2,\n",
       " 'transform': 1,\n",
       " 'vérit': 3,\n",
       " 'chass': 1,\n",
       " 'énigm': 1,\n",
       " 'doit': 1,\n",
       " 'compt': 2,\n",
       " 'pos': 1,\n",
       " 'cet': 3,\n",
       " 'semblabl': 1,\n",
       " 'temp': 3,\n",
       " 'éloign': 1,\n",
       " 'nôtr': 1,\n",
       " 'ensuit': 1,\n",
       " 'fair': 2,\n",
       " 'bascul': 1,\n",
       " 'cœur': 1,\n",
       " 'intrigu': 1,\n",
       " 'suiv': 2,\n",
       " 'natur': 1,\n",
       " 'chacun': 1,\n",
       " 'précisent': 1,\n",
       " 'enjeux': 1,\n",
       " 'font': 1,\n",
       " 'cours': 1,\n",
       " 'influenc': 2,\n",
       " 'oeuvr': 1,\n",
       " 'j.': 1,\n",
       " 'k.': 1,\n",
       " 'rowling': 3,\n",
       " 'dit': 3,\n",
       " 'certain': 1,\n",
       " 'aspect': 1,\n",
       " 'plair': 1,\n",
       " 'aim': 6,\n",
       " 'harry': 6,\n",
       " 'pott': 5,\n",
       " 'seigneur': 1,\n",
       " 'anneau': 1,\n",
       " 'twilight': 1,\n",
       " 'eragon': 1,\n",
       " 'autr': 4,\n",
       " 'fantast': 2,\n",
       " 'cach': 1,\n",
       " 'multipl': 1,\n",
       " 'empreint': 1,\n",
       " 'mag': 1,\n",
       " 'suspen': 1,\n",
       " 'trahison': 1,\n",
       " 'quoi': 1,\n",
       " 'mieux': 1,\n",
       " 'avoir': 4,\n",
       " 'inform': 1,\n",
       " 'expliqu': 1,\n",
       " 'faison': 1,\n",
       " 'chos': 2,\n",
       " 'moiti': 1,\n",
       " 'gazet': 1,\n",
       " 'contact': 1,\n",
       " 'eu': 1,\n",
       " 'gentilless': 1,\n",
       " 'répondr': 1,\n",
       " 'quelqu': 2,\n",
       " 'question': 1,\n",
       " 'entretien': 1,\n",
       " 'c’': 6,\n",
       " 'j.k.': 2,\n",
       " 'don': 4,\n",
       " 'envi': 2,\n",
       " 'écrir': 3,\n",
       " 'quel': 1,\n",
       " 'plac': 1,\n",
       " 'tient': 1,\n",
       " 'votr': 1,\n",
       " 'lis': 2,\n",
       " 'étais': 2,\n",
       " 'ailleur': 1,\n",
       " 'persuad': 1,\n",
       " 'ça': 2,\n",
       " 'avais': 3,\n",
       " 'encor': 2,\n",
       " 'm’': 5,\n",
       " 'intéress': 2,\n",
       " 'savoir': 3,\n",
       " 'lu': 1,\n",
       " 'mon': 5,\n",
       " 'préfer': 1,\n",
       " 'not': 2,\n",
       " 'parc': 3,\n",
       " 'particuli': 1,\n",
       " 'émot': 1,\n",
       " 'pouv': 2,\n",
       " 'rir': 1,\n",
       " 'cou': 1,\n",
       " 'pétuni': 1,\n",
       " 'maladress': 1,\n",
       " 'hagrid': 1,\n",
       " 'frisson': 1,\n",
       " 'détraqueur': 1,\n",
       " 'mélancol': 1,\n",
       " 'parent': 1,\n",
       " 'sort': 3,\n",
       " 'réserv': 1,\n",
       " 'buck': 1,\n",
       " '5': 1,\n",
       " 'fallu': 1,\n",
       " 'attendr': 1,\n",
       " 'an': 1,\n",
       " 'prochain': 1,\n",
       " 'rien': 1,\n",
       " 'me': 10,\n",
       " 'mettr': 1,\n",
       " 'sous': 1,\n",
       " 'dent': 1,\n",
       " 'alor': 2,\n",
       " 'fiction': 1,\n",
       " 'écrit': 1,\n",
       " 'peut': 1,\n",
       " 'internet': 1,\n",
       " 'séduit': 2,\n",
       " 'accueil': 1,\n",
       " 'recev': 1,\n",
       " 'travail': 1,\n",
       " 'amateur': 1,\n",
       " 'souvent': 3,\n",
       " 'hauteur': 1,\n",
       " 'toujour': 2,\n",
       " 'bienvenu': 1,\n",
       " 'discutent': 1,\n",
       " 'bienveil': 1,\n",
       " 'conseil': 1,\n",
       " 'critiqu': 1,\n",
       " 'construct': 1,\n",
       " 'façon': 3,\n",
       " 'suis': 3,\n",
       " 'lanc': 2,\n",
       " 'commenc': 3,\n",
       " 'écriv': 1,\n",
       " 'rendr': 1,\n",
       " 'invent': 1,\n",
       " 'grecqu': 1,\n",
       " 'omniprésent': 1,\n",
       " 'nom': 1,\n",
       " 'prophet': 2,\n",
       " 'général': 2,\n",
       " 'présent': 3,\n",
       " 'entourent': 1,\n",
       " 'them': 1,\n",
       " 'passion': 1,\n",
       " 'parallel': 1,\n",
       " 'entre': 2,\n",
       " 'surtout': 1,\n",
       " 'côt': 1,\n",
       " 'retord': 1,\n",
       " 'lesquel': 1,\n",
       " 'pauvr': 1,\n",
       " 'mortel': 1,\n",
       " 'pouvon': 1,\n",
       " 'grand-chos': 1,\n",
       " 'sourc': 1,\n",
       " 'inépuis': 1,\n",
       " 'leçon': 1,\n",
       " 'moral': 1,\n",
       " 'tram': 1,\n",
       " '!': 2,\n",
       " 'plateform': 2,\n",
       " 'plutôt': 1,\n",
       " 'maison': 3,\n",
       " 'traditionnel': 1,\n",
       " 'sortent': 1,\n",
       " 'effect': 1,\n",
       " 'ulul': 2,\n",
       " 'non': 1,\n",
       " 'raison': 1,\n",
       " 'divers': 1,\n",
       " 'pass': 1,\n",
       " 'sav': 1,\n",
       " 'comment': 1,\n",
       " 'cel': 2,\n",
       " 'fonction': 1,\n",
       " 'entendu': 1,\n",
       " 'était': 2,\n",
       " 'processus': 1,\n",
       " 'long': 1,\n",
       " 'part': 2,\n",
       " 'méti': 1,\n",
       " 'graphist': 1,\n",
       " 'voul': 1,\n",
       " 'occup': 1,\n",
       " 'moi': 4,\n",
       " 'commun': 1,\n",
       " 'mar': 1,\n",
       " 'avion': 1,\n",
       " 'décid': 2,\n",
       " 'projet': 2,\n",
       " 'publiqu': 1,\n",
       " 'ressourc': 1,\n",
       " 'si': 1,\n",
       " 'suscit': 1,\n",
       " 'intérêt': 1,\n",
       " 'aupres': 1,\n",
       " 'savion': 1,\n",
       " 'refair': 1,\n",
       " 'anné': 1,\n",
       " 'sembl': 1,\n",
       " 'évident': 1,\n",
       " 'gagn': 2,\n",
       " 'puisqu': 1,\n",
       " 'auteur': 1,\n",
       " 'géner': 1,\n",
       " 'nécessair': 1,\n",
       " 'reçoivent': 1,\n",
       " 'produit': 1,\n",
       " 'avant-premi': 1,\n",
       " 'bénéficient': 1,\n",
       " 'promot': 1,\n",
       " 'moment': 1,\n",
       " 'échang': 1,\n",
       " 'partag': 1,\n",
       " 'cadeau': 1,\n",
       " 'bonus': 2,\n",
       " 'débloqu': 2,\n",
       " 'dep': 1,\n",
       " 'pali': 1,\n",
       " 'exempl': 1,\n",
       " 'derni': 1,\n",
       " 'chapitr': 1,\n",
       " 'dévoil': 1,\n",
       " 'noël': 1,\n",
       " 'éprouv': 1,\n",
       " 'craint': 1,\n",
       " 'réjouiss': 1,\n",
       " 'grand': 1,\n",
       " 'manqu': 1,\n",
       " 'confianc': 1,\n",
       " 'pouss': 1,\n",
       " 'pir': 1,\n",
       " 'bel': 1,\n",
       " 'dessin': 1,\n",
       " 'fier': 2,\n",
       " 'lieu': 1,\n",
       " 'laiss': 1,\n",
       " 'exprim': 1,\n",
       " 'ma': 1,\n",
       " 'réaction': 1,\n",
       " 'inquiet': 1,\n",
       " 'achet': 1,\n",
       " 'lui': 1,\n",
       " 'plait': 1,\n",
       " 'puiss': 1,\n",
       " 'déçu': 1,\n",
       " 'heureux': 3,\n",
       " 'appris': 1,\n",
       " 'cas': 1,\n",
       " 'vrai': 2,\n",
       " 'lectorat': 1,\n",
       " 'peux': 1,\n",
       " 'dir': 1,\n",
       " 'sais': 1,\n",
       " 'rendent': 1,\n",
       " 'sensat': 1,\n",
       " 'magiqu': 1,\n",
       " 'gen': 1,\n",
       " 'ri': 1,\n",
       " 'pleur': 1,\n",
       " 'sent': 1,\n",
       " 'auss': 1,\n",
       " 'util': 1,\n",
       " 'remerci': 1,\n",
       " 'assez': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fonctionne, mais doit tourner sur plusieurs textes pour etre efficace ?\n",
    "def word_in_text(texte):\n",
    "    \"\"\"Renvoie un dictionnaire avec en cle\n",
    "    les mots du texte et en valeur le nombre d'occurence\"\"\"\n",
    "    dictionnaire = {}\n",
    "    n = len(tokens)\n",
    "    tokens = return_stem(texte) #Recuperation des racines des tokens\n",
    "    for i in range(len(tokens)): #Pour tout les tokens \n",
    "        if (tokens[i] in dictionnaire):\n",
    "            dictionnaire[tokens[i]] = dictionnaire[tokens[i]] + 1\n",
    "        else:\n",
    "            dictionnaire[tokens[i]] = 1\n",
    "    return dictionnaire\n",
    "\n",
    "occ_tot = word_in_text(prelat)\n",
    "occ_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction initiale\n",
    "def return_sentence_emb(sentence):\n",
    "    \"\"\"Prend en entree une phrase vectorise mot a mot\"\"\"\n",
    "    somme = sentence[0]\n",
    "    for i in range(len(sentence)) :\n",
    "        if i != 0 :\n",
    "            somme += sentence[i]\n",
    "    return somme/len(sentence)\n",
    "\n",
    "Vect = return_sentence_emb(Sentence_emb0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
