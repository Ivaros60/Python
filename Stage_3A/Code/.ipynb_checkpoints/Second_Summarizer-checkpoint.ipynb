{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run First_Summarizer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation d'un modèle pour la langue francaise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "path = \"C:\\\\Users\\\\Ivaros\\\\Documents\\\\Python\\\\NLP\\\\\" #A modifier/automatiser\n",
    "nlp = spacy.load(path + \"fr_core_news_sm-2.2.5\") #Charge le modele fr deja installe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Ivaros\\\\Documents\\\\Python\\\\Stage_3A\\\\Debat\\\\Interview_Prelats.txt', 'r', encoding = \"utf-8\") as file:\n",
    "     prelat = file.read() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Améliorations du Premier Summarizer\n",
    "\n",
    "### Sur la Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Seconde methode pour Tokeniser. fonction sent_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download()\n",
    "Tokenize = sent_tokenize(prelat, language = \"french\")\n",
    "Tokenize\n",
    "len(Tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenisation semble de meilleurs qualités !\n",
    "\n",
    "## Suppression des StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_supp =[\"Bonjour\", \"bonjour\"] #Liste d'element a supprimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai',\n",
       " 'aie',\n",
       " 'aient',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'as',\n",
       " 'au',\n",
       " 'aura',\n",
       " 'aurai',\n",
       " 'auraient',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'auras',\n",
       " 'aurez',\n",
       " 'auriez',\n",
       " 'aurions',\n",
       " 'aurons',\n",
       " 'auront',\n",
       " 'aux',\n",
       " 'avaient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avec',\n",
       " 'avez',\n",
       " 'aviez',\n",
       " 'avions',\n",
       " 'avons',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'ayez',\n",
       " 'ayons',\n",
       " 'c',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'd',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eurent',\n",
       " 'eus',\n",
       " 'eusse',\n",
       " 'eussent',\n",
       " 'eusses',\n",
       " 'eussiez',\n",
       " 'eussions',\n",
       " 'eut',\n",
       " 'eux',\n",
       " 'eûmes',\n",
       " 'eût',\n",
       " 'eûtes',\n",
       " 'furent',\n",
       " 'fus',\n",
       " 'fusse',\n",
       " 'fussent',\n",
       " 'fusses',\n",
       " 'fussiez',\n",
       " 'fussions',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fût',\n",
       " 'fûtes',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'j',\n",
       " 'je',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'même',\n",
       " 'n',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ont',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 's',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'sera',\n",
       " 'serai',\n",
       " 'seraient',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'seras',\n",
       " 'serez',\n",
       " 'seriez',\n",
       " 'serions',\n",
       " 'serons',\n",
       " 'seront',\n",
       " 'ses',\n",
       " 'soient',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'sommes',\n",
       " 'son',\n",
       " 'sont',\n",
       " 'soyez',\n",
       " 'soyons',\n",
       " 'suis',\n",
       " 'sur',\n",
       " 't',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'y',\n",
       " 'à',\n",
       " 'étaient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étantes',\n",
       " 'étants',\n",
       " 'étiez',\n",
       " 'étions',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'êtes'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour supprimer les stopWords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('french'))\n",
    "\n",
    "def delete_SW(test,stopWords) :\n",
    "    clean_words = []\n",
    "    for token in return_token(test): #return_token tokenize mot a mot\n",
    "        if token not in stopWords:\n",
    "            clean_words.append(token)\n",
    "    return clean_words\n",
    "\n",
    "#clean_words = delete_SW(prelat,stopWords)\n",
    "#clean_words #Renvoie une tokenisation mot a mot\n",
    "\n",
    "#Ajout d'elements a SW\n",
    "\n",
    "def add_SW(L,SW) :\n",
    "    \"\"\"Ajoute une liste d element\"\"\"\n",
    "    for j in L:\n",
    "       SW.add(j) \n",
    "    return SW\n",
    "\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relancer la fonction de resume avec les cgts\n",
    "\n",
    "def return_text_emb(texte) : \n",
    "    \"\"\"Vectorisation du texte\"\"\"\n",
    "    text_vect = []\n",
    "    Tokenize = sent_tokenize(texte, language = \"french\") #Tokenisation par phrase    \n",
    "    for i in range(len(Tokenize)) :\n",
    "        Sentence_emb = return_word_embedding(Tokenize[i]) #Vectorisation des phrases\n",
    "        vect = return_sentence_emb(Sentence_emb)\n",
    "        text_vect += [vect]\n",
    "    return text_vect\n",
    "\n",
    "Test = return_text_emb(prelat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pondération de la phrase\n",
    "\n",
    "On va chercher a calculer la frequence d'apparition de chaque mot dans notre texte pour donner de l importance aux mots \"rares\"\n",
    "\n",
    "### Obtention de la fréquence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération des racines de mots\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='french')\n",
    "\n",
    "def return_stem(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [stemmer.stem(X.text) for X in doc] #X.text correspond au mot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obt_occ(dictionnaire,stopwords,mot) :\n",
    "    \"\"\"Renvoie le nombre d'occurence du mot \"\"\"\n",
    "    if mot in stopwords :\n",
    "        return 0\n",
    "    else : \n",
    "        if (mot in dictionnaire) :\n",
    "            return 1/dictionnaire[mot]\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009453781512605041"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fonctionne, mais doit tourner sur plusieurs textes pour etre efficace ?\n",
    "def word_in_text(texte):\n",
    "    \"\"\"Renvoie un dictionnaire avec en cle\n",
    "    les mots du texte et en valeur le nombre d'occurence\"\"\"\n",
    "    dictionnaire = {}\n",
    "    tokens = return_stem(texte) #Recuperation des racines des tokens\n",
    "    n = len(tokens)\n",
    "    for i in range(len(tokens)): #Pour tout les tokens \n",
    "        if (tokens[i] in dictionnaire):\n",
    "            dictionnaire[tokens[i]] = dictionnaire[tokens[i]] + (1/n)\n",
    "        else:\n",
    "            dictionnaire[tokens[i]] = (1/n)\n",
    "    return dictionnaire\n",
    "\n",
    "occ_tot = word_in_text(prelat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculer l'embedding de la phrase a partir des poids. poids x representation a partir de la racine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence correspond a la str de la phrase\n",
    "def calc_emb_sent(sentence,occ_tot):\n",
    "    \"\"\"Prend en entree une phrase vectorise mot a mot et renvoie une avec ponderation\"\"\"\n",
    "    sentence_emb = return_word_embedding(sentence)\n",
    "    somme = sentence_emb[0] - sentence_emb[0] #Initialisation d'un tableau vide\n",
    "    tokens = return_stem(sentence) #Recuperation des racines des tokens\n",
    "    n = len(tokens)\n",
    "    for i in range(len(tokens)): #Pour tout les tokens\n",
    "        poids = occ_tot[str(tokens[i])]\n",
    "        somme += poids * sentence_emb[i]\n",
    "    return somme/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_emb_text(texte,occ_tot):\n",
    "    \"\"\"texte en entree et renvoie l embedding du texte avec ponderation\"\"\"\n",
    "    text = []\n",
    "    Tokenize = sent_tokenize(texte, language = \"french\")\n",
    "    for i in range(len(Tokenize)): \n",
    "        text += [calc_emb_sent(Tokenize[i],occ_tot)]\n",
    "    return text\n",
    "        \n",
    "A = calc_emb_text(prelat,occ_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterisation2(texte, occ_tot, percent = 0.1) :\n",
    "    \"\"\"K_means sur le texte, percent correspond au pourcentage du texte que l'on veut garder\"\"\"\n",
    "    #Obtention du nombre de phrase\n",
    "    Tokenize = sent_tokenize(texte, language = \"french\")\n",
    "    nb_phrase = len(Tokenize)\n",
    "    nb_centre = int(np.ceil(percent*nb_phrase))\n",
    "    #Representation vectorielle du texte phrase par phrase\n",
    "    emb_text = calc_emb_text(texte,occ_tot)\n",
    "    #Kmeans\n",
    "    kmeans = KMeans(n_clusters=nb_centre)\n",
    "    kmeans = kmeans.fit(emb_text)\n",
    "    return (kmeans, nb_centre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary2(texte, occ_tot) :\n",
    "    \"\"\"Renvoie le resume\"\"\"\n",
    "    clust = clusterisation2(texte, occ_tot)\n",
    "    kmeans  = clust[0]\n",
    "    nb_centre = clust[1]\n",
    "    avg = []\n",
    "    for j in range(nb_centre):\n",
    "        idx = np.where(kmeans.labels_ == j)[0]\n",
    "        avg.append(np.mean(idx))\n",
    "    Tokenize = sent_tokenize(texte, language = \"french\")\n",
    "    emb_text = calc_emb_text(texte,occ_tot)\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, emb_text)\n",
    "    ordering = sorted(range(nb_centre), key=lambda k: avg[k])\n",
    "    summary = ' '.join([Tokenize[closest[idx]] for idx in ordering])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L’INFLUENCE DE L’OEUVRE DE J. K. ROWLING\\nL’autrice le dit elle-même sur son site, «par certains aspects, cet univers pourrait plaire à ceux qui auraient aimé Harry Potter, Le Seigneur des anneaux, Twilight, Eragon… et beaucoup d’autres.» Car en plus de nous plonger dans un univers fantastique qui cache de multiples secrets, les livres de Charlotte Abécassis Weigel sont empreints de magie, de suspens, de trahisons… Et quoi de mieux pour avoir des informations sur un roman que d’avoir les explications de son autrice ? J’ai alors découvert les fan-fictions, des histoires écrites par des fans que l’on peut trouver sur Internet. Il y en a beaucoup sur Harry Potter et j’ai tout de suite été séduite par l’accueil qu’elles recevaient. J’ai commencé en écrivant sur Harry Potter, puis, sans même m’en rendre compte, j’ai commencé à inventer mon univers. J’ai d’abord été attirée par la couverture très sobre du premier tome, un petit renard stylisé sur fond bleu, puis une fois le livre en main, j’ai tout de suite été captivée par les trois premières phrases :\\n\\n«Vous qui n’avez ni dieu, ni pouvoir. On comprend rapidement que chaque petit détail a son importance et le roman se transforme ainsi en une véritable chasse aux énigmes. Les raisons sont diverses. Crainte, réjouissance ?'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = print_summary2(prelat,occ_tot)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aujourd’hui, je viens vous parler d’une saga d’heroic fantasy découverte lors de l’édition 2019 du Salon du Livre de Paris, Les Prélats de Faneas de Charlotte Abécassis Weigel.\\n Je pouvais rire en lisant les descriptions du cou de Pétunia ou les maladresses d’Hagrid, et je pouvais frissonner en découvrant les détraqueurs, l’ En effet, l’autrice prend un malin plaisir à mener ses personnages, ainsi que ses lecteurs en bateau. Les raisons sont diverses.\\n mélancolique des parents de Harry ou encore le sort réservé à Buck.\\n De plus, c’est un temps fort dans la vie du livre, un moment d’échange et de partage, entre autre grâce aux cadeaux bonus débloqués à chaque fois que l’on dépasse un nouveau palier. Mes livres sortent effectivement via des financements participatifs sur la plateforme Ulule et non grâce à une maison d’édition. Crainte, réjouissance ?'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_summary(prelat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'amélioration du process est concluante. Le résumé semble de meilleur qualité (Premier résumé). Cependant la mise en forme est à revoir !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
